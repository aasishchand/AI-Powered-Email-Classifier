# Big Data Stack - Environment Template
# Copy to .env and fill values when running docker-compose with big data services.

# --- MinIO (S3-compatible) ---
MINIO_ROOT_USER=minioadmin
MINIO_ROOT_PASSWORD=minioadmin
MINIO_BUCKET_RAW=emails-raw
MINIO_BUCKET_PROCESSED=emails-processed
MINIO_BUCKET_MODELS=email-models

# --- Spark ---
SPARK_MASTER_URL=spark://spark-master:7077

# --- Hive Metastore ---
HIVE_METASTORE_URI=thrift://hive-metastore:9083

# --- Kafka ---
KAFKA_BOOTSTRAP_SERVERS=kafka:9092
KAFKA_TOPIC_EMAILS_RAW=emails-raw
KAFKA_TOPIC_EMAILS_CLASSIFIED=emails-classified

# --- Airflow ---
AIRFLOW_UID=50000
AIRFLOW_ADMIN_USER=admin
AIRFLOW_ADMIN_PASSWORD=admin
AIRFLOW__CORE__EXECUTOR=CeleryExecutor
AIRFLOW__CELERY__BROKER_URL=redis://redis:6379/1
AIRFLOW__DATABASE__SQL_ALCHEMY_CONN=postgresql+psycopg2://admin:secret@postgres:5432/airflow
AIRFLOW__CELERY__RESULT_BACKEND=db+postgresql://admin:secret@postgres:5432/airflow

# --- PostgreSQL (app + Hive metastore + Airflow) ---
POSTGRES_USER=admin
POSTGRES_PASSWORD=secret
POSTGRES_DB=email_platform

# --- Optional: Slack for model training notifications ---
SLACK_WEBHOOK_URL=

# --- FastAPI backend (for notify_api_task) ---
BACKEND_URL=http://backend:8000
