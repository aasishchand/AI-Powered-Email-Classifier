version: '3.8'

services:
  # --- existing app services ---
  frontend:
    build:
      context: ./faculty-email-dashboard
      dockerfile: Dockerfile
    ports:
      - "3000:80"
    environment:
      - VITE_API_URL=http://localhost:8000
      - VITE_WS_URL=ws://localhost:8000
    depends_on:
      - backend
    networks:
      - email_platform

  backend:
    build:
      context: ./email-api
      dockerfile: Dockerfile
    ports:
      - "8000:8000"
    environment:
      - DATABASE_URL=postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-secret}@postgres:5432/${POSTGRES_DB:-email_platform}
      - REDIS_URL=redis://redis:6379/0
      - JWT_SECRET_KEY=${JWT_SECRET_KEY:-your-secret-key-change-in-production}
      - ENVIRONMENT=production
      - ALLOWED_ORIGINS=http://localhost:3000,http://localhost:80
      - MINIO_ENDPOINT=http://minio:9000
      - MINIO_ACCESS_KEY=${MINIO_ROOT_USER:-minioadmin}
      - MINIO_SECRET_KEY=${MINIO_ROOT_PASSWORD:-minioadmin}
      - MINIO_BUCKET_RAW=${MINIO_BUCKET_RAW:-emails-raw}
      - MINIO_BUCKET_PROCESSED=${MINIO_BUCKET_PROCESSED:-emails-processed}
      - AIRFLOW_WEBSERVER_URL=http://airflow-webserver:8085
    volumes:
      - ./email-api:/app
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    networks:
      - email_platform
      - bigdata-net
    restart: unless-stopped

  postgres:
    image: postgres:15-alpine
    environment:
      POSTGRES_DB: ${POSTGRES_DB:-email_platform}
      POSTGRES_USER: ${POSTGRES_USER:-admin}
      POSTGRES_PASSWORD: ${POSTGRES_PASSWORD:-secret}
    ports:
      - "5432:5432"
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./sql/init.sql:/docker-entrypoint-initdb.d/init.sql
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U ${POSTGRES_USER:-admin} -d ${POSTGRES_DB:-email_platform}"]
      interval: 5s
      timeout: 5s
      retries: 5
    networks:
      - email_platform
      - bigdata-net
    restart: unless-stopped

  redis:
    image: redis:7-alpine
    ports:
      - "6379:6379"
    volumes:
      - redis_data:/data
    networks:
      - email_platform
      - bigdata-net
    restart: unless-stopped

  nginx:
    image: nginx:alpine
    ports:
      - "80:80"
    volumes:
      - ./nginx/nginx.conf:/etc/nginx/nginx.conf:ro
    depends_on:
      - frontend
      - backend
    networks:
      - email_platform
    restart: unless-stopped

  # --- big data: MinIO ---
  minio:
    image: minio/minio:latest
    command: server /data --console-address ":9001"
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
    ports:
      - "9000:9000"
      - "9001:9001"
    volumes:
      - minio_data:/data
    healthcheck:
      test: ["CMD", "mc", "ready", "local"]
      interval: 10s
      timeout: 5s
      retries: 3
      start_period: 10s
    networks:
      - bigdata-net
    restart: unless-stopped

  minio-init:
    image: minio/mc:latest
    depends_on:
      minio:
        condition: service_healthy
    entrypoint: >
      /bin/sh -c "
      mc alias set myminio http://minio:9000 $${MINIO_ROOT_USER} $${MINIO_ROOT_PASSWORD};
      mc mb myminio/$${MINIO_BUCKET_RAW} --ignore-existing;
      mc mb myminio/$${MINIO_BUCKET_PROCESSED} --ignore-existing;
      mc mb myminio/$${MINIO_BUCKET_MODELS} --ignore-existing;
      exit 0;
      "
    environment:
      MINIO_ROOT_USER: ${MINIO_ROOT_USER:-minioadmin}
      MINIO_ROOT_PASSWORD: ${MINIO_ROOT_PASSWORD:-minioadmin}
      MINIO_BUCKET_RAW: ${MINIO_BUCKET_RAW:-emails-raw}
      MINIO_BUCKET_PROCESSED: ${MINIO_BUCKET_PROCESSED:-emails-processed}
      MINIO_BUCKET_MODELS: ${MINIO_BUCKET_MODELS:-email-models}
    networks:
      - bigdata-net
    restart: "no"

  # --- Spark ---
  spark-master:
    image: bitnami/spark:3.5
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8181
    ports:
      - "8181:8181"
      - "7077:7077"
    volumes:
      - spark_logs:/opt/bitnami/spark/logs
    healthcheck:
      test: ["CMD", "curl", "-f", "http://localhost:8181/"]
      interval: 15s
      timeout: 5s
      retries: 3
      start_period: 30s
    networks:
      - bigdata-net
    restart: unless-stopped

  spark-worker-1:
    image: bitnami/spark:3.5
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 2G
    volumes:
      - spark_logs:/opt/bitnami/spark/logs
    depends_on:
      - spark-master
    networks:
      - bigdata-net
    restart: unless-stopped

  spark-worker-2:
    image: bitnami/spark:3.5
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_MEMORY: 2G
    volumes:
      - spark_logs:/opt/bitnami/spark/logs
    depends_on:
      - spark-master
    networks:
      - bigdata-net
    restart: unless-stopped

  # --- Hive Metastore ---
  hive-metastore:
    image: apache/hive:3.1.3
    environment:
      DB_DRIVER: postgres
      SERVICE_NAME: metastore
      IS_RESUME: "true"
      DB_OPTIONS: "-Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/hive_metastore -Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionUserName=$${POSTGRES_USER} -Djavax.jdo.option.ConnectionPassword=$${POSTGRES_PASSWORD}"
    depends_on:
      postgres:
        condition: service_healthy
    networks:
      - bigdata-net
    restart: unless-stopped

  # --- Kafka (KRaft, no ZooKeeper) ---
  kafka:
    image: bitnami/kafka:3.7
    environment:
      KAFKA_CFG_NODE_ID: 1
      KAFKA_CFG_PROCESS_ROLES: controller,broker
      KAFKA_CFG_LISTENERS: PLAINTEXT://:9092,CONTROLLER://:9093,EXTERNAL://:29092
      KAFKA_CFG_ADVERTISED_LISTENERS: PLAINTEXT://kafka:9092,EXTERNAL://localhost:29092
      KAFKA_CFG_CONTROLLER_QUORUM_VOTERS: 1@kafka:9093
    ports:
      - "29092:29092"
    volumes:
      - kafka_data:/bitnami/kafka
    healthcheck:
      test: ["CMD-SHELL", "kafka-topics.sh --bootstrap-server localhost:9092 --list || exit 1"]
      interval: 10s
      timeout: 10s
      retries: 5
      start_period: 30s
    networks:
      - bigdata-net
    restart: unless-stopped

  kafka-ui:
    image: provectuslabs/kafka-ui:latest
    ports:
      - "8080:8080"
    environment:
      KAFKA_CLUSTERS_0_NAME: local
      KAFKA_CLUSTERS_0_BOOTSTRAPSERVERS: kafka:9092
      KAFKA_CLUSTERS_0_ZOOKEEPER: ""
    depends_on:
      - kafka
    networks:
      - bigdata-net
    restart: unless-stopped

  # --- Airflow ---
  airflow-init:
    image: apache/airflow:2.9.1
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID:-50000}
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-secret}@postgres:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-secret}@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/1
      _AIRFLOW_WWW_USER_USERNAME: ${AIRFLOW_ADMIN_USER:-admin}
      _AIRFLOW_WWW_USER_PASSWORD: ${AIRFLOW_ADMIN_PASSWORD:-admin}
    volumes:
      - airflow_logs:/opt/airflow/logs
      - ./data-pipeline/airflow/dags:/opt/airflow/dags
      - ./data-pipeline/airflow/plugins:/opt/airflow/plugins
    command: >
      bash -c "
      airflow db init &&
      airflow users create --username $${_AIRFLOW_WWW_USER_USERNAME} --password $${_AIRFLOW_WWW_USER_PASSWORD} --firstname Admin --lastname User --role Admin --email admin@example.com || true
      "
    depends_on:
      postgres:
        condition: service_healthy
      redis:
        condition: service_started
    networks:
      - bigdata-net
    restart: "no"

  airflow-webserver:
    image: apache/airflow:2.9.1
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID:-50000}
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-secret}@postgres:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-secret}@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/1
      AIRFLOW__WEBSERVER__EXPOSE_CONFIG: "true"
    ports:
      - "8085:8080"
    volumes:
      - airflow_logs:/opt/airflow/logs
      - ./data-pipeline/airflow/dags:/opt/airflow/dags
      - ./data-pipeline/airflow/plugins:/opt/airflow/plugins
      - ./data-pipeline/spark:/opt/airflow/spark
    command: webserver
    depends_on:
      - airflow-init
    networks:
      - bigdata-net
    restart: unless-stopped

  airflow-scheduler:
    image: apache/airflow:2.9.1
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID:-50000}
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-secret}@postgres:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-secret}@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/1
    volumes:
      - airflow_logs:/opt/airflow/logs
      - ./data-pipeline/airflow/dags:/opt/airflow/dags
      - ./data-pipeline/airflow/plugins:/opt/airflow/plugins
      - ./data-pipeline/spark:/opt/airflow/spark
    command: scheduler
    depends_on:
      - airflow-init
    networks:
      - bigdata-net
    restart: unless-stopped

  airflow-worker:
    image: apache/airflow:2.9.1
    environment:
      AIRFLOW_UID: ${AIRFLOW_UID:-50000}
      AIRFLOW__CORE__EXECUTOR: CeleryExecutor
      AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-secret}@postgres:5432/airflow
      AIRFLOW__CELERY__RESULT_BACKEND: db+postgresql://${POSTGRES_USER:-admin}:${POSTGRES_PASSWORD:-secret}@postgres:5432/airflow
      AIRFLOW__CELERY__BROKER_URL: redis://redis:6379/1
    volumes:
      - airflow_logs:/opt/airflow/logs
      - ./data-pipeline/airflow/dags:/opt/airflow/dags
      - ./data-pipeline/airflow/plugins:/opt/airflow/plugins
      - ./data-pipeline/spark:/opt/airflow/spark
    command: celery worker
    depends_on:
      - airflow-init
    networks:
      - bigdata-net
    restart: unless-stopped

volumes:
  postgres_data:
  redis_data:
  minio_data:
  spark_logs:
  airflow_logs:
  kafka_data:

networks:
  email_platform:
    driver: bridge
  bigdata-net:
    driver: bridge